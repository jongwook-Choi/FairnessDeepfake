# Stage 2 Independent Adapter 하이퍼파라미터 튜닝 보고서

## 1. 실험 개요

### 1.1 목적
CLIP 기반 Deepfake Detection 모델의 Stage 2 Independent Adapter 학습에서,
**일반화 성능(AUC)**과 **공정성(Fairness)**의 최적 균형점을 찾기 위한 3회 반복 하이퍼파라미터 튜닝.

### 1.2 실험 환경
- **모델**: CLIP ViT-L/14 (Frozen) + Stage 1 Adapter (Frozen) + Stage 2 Adapter + Cross-Attention Fusion + Binary Classifier
- **학습 데이터**: FF++ (FaceForensics++)
- **검증 데이터**: FF++ (validation split)
- **테스트 데이터**: CelebDF, DFD, DFDC (cross-dataset generalization 평가)
- **총 파라미터**: ~353M (Trainable: ~9.4M / 2.66%, Frozen: ~343.6M / 97.34%)
- **GPU**: CUDA
- **총 실험 시간**: ~21.7시간 (Exp1: 7.71h + Exp2: 6.73h + Exp3: ~7.3h)

### 1.3 평가 지표
| 카테고리 | 지표 | 설명 |
|---------|------|------|
| 탐지 성능 | AUC | Area Under ROC Curve (주요 지표) |
| 탐지 성능 | ACC | Accuracy |
| 탐지 성능 | EER | Equal Error Rate (낮을수록 좋음) |
| 탐지 성능 | AP | Average Precision |
| 공정성 | F_FPR | Fairness of False Positive Rate (낮을수록 공정) |
| 공정성 | F_OAE | Overall Accuracy Equality (낮을수록 공정) |
| 공정성 | F_DP | Demographic Parity (낮을수록 공정) |
| 공정성 | F_MEO | Maximum Equalized Odds (낮을수록 공정) |

---

## 2. 실험별 설정 비교

### 2.1 하이퍼파라미터 변경 이력

| 파라미터 | Exp1 (Baseline) | Exp2 (Fairness Boost) | Exp3 (Balanced Recovery) |
|---------|-----------------|----------------------|------------------------|
| lr | 0.0003 | 0.0002 (↓) | **0.0003** (복원) |
| weight_decay | 0.01 | 0.05 (↑) | **0.01** (복원) |
| warmup_epochs | 3 | 2 (↓) | **3** (복원) |
| num_epochs | 30 | 25 (↓) | **30** (복원) |
| dropout | 0.1 | 0.2 (↑) | **0.15** (절충) |
| label_smoothing | 0.0 | 0.1 (추가) | **0.1** (유지) |
| lambda_fair | 0.05 | 0.5 (10×↑) | **0.15** (3×↑) |
| sinkhorn_blur | 1e-4 | 1e-3 (10×↑) | **1e-4** (복원) |
| gate_init_bias | 1.0 | 1.0 (유지) | **0.0** (변경) |

### 2.2 설계 근거

#### Exp2: Fairness 강화 시도
- lambda_fair 10배 증가로 공정성 손실 비중 확대 의도
- 정규화 강화 (dropout, weight_decay, label_smoothing)로 일반화 개선 시도

#### Exp3: 균형 복원
- **핵심 발견**: sinkhorn_blur 증가가 fair loss 크기를 ~8배 축소시켜 lambda_fair 증가를 완전히 상쇄
- 성능 저하 요인 제거: 과도한 정규화(weight_decay 0.05) 복원
- gate_init_bias 0.0으로 변경: sigmoid(0.0)=0.5에서 출발하여 gate 포화 방지
- label_smoothing은 일반화 효과가 있어 유지

---

## 3. 실험 결과

### 3.1 핵심 성능 지표 비교

| 지표 | Exp1 (Baseline) | Exp2 (Fairness Boost) | Exp3 (Balanced Recovery) |
|------|-----------------|----------------------|------------------------|
| **Best Val AUC** | 0.8270 (Ep24) | 0.8111 (Ep22) | **0.8232 (Ep25)** |
| Best Val ACC | 0.8160 | 0.7893 | 0.7772 |
| Best Val EER | 0.2541 | 0.2766 | 0.2593 |
| Best Val AP | 0.9618 | - | 0.9606 |
| Train Loss (final) | 0.2737 | 0.4216 | 0.3833 |
| Degenerate Phase | Ep 1-5 | Ep 1-11 | Ep 1-5 |
| 학습 시간 | 7.71h | 6.73h | ~7.3h |

### 3.2 Cross-Dataset 일반화 성능 (Best Model 기준 Test AUC)

| 데이터셋 | Exp1 (Ep24) | Exp2 (Ep22) | Exp3 (Ep25) |
|---------|-------------|-------------|-------------|
| CelebDF | **0.6401** | 0.5910 | 0.6247 |
| DFD | **0.8119** | 0.7860 | 0.7914 |
| DFDC | 0.7058 | **0.7092** | 0.7050 |
| **평균** | **0.7193** | 0.6954 | 0.7070 |

### 3.3 Validation Fairness 지표 (Best Epoch 기준)

| 지표 | Exp1 (Ep24) | Exp2 (Ep22) | Exp3 (Ep25) | 방향 |
|------|-------------|-------------|-------------|------|
| F_FPR | 166.89% | 176.36% | **155.23%** | ↓ 최선 |
| F_OAE | **13.95%** | 18.29% | 15.77% | ↓ 최선 |
| F_DP | **38.71%** | 51.63% | 47.17% | ↓ 최선 |
| F_MEO | **92.17%** | 94.42% | **87.75%** | ↓ 최선 |

### 3.4 Test Fairness 지표 (Best Model 기준)

#### CelebDF
| 지표 | Exp1 (Ep24) | Exp2 (Ep22) | Exp3 (Ep25) |
|------|-------------|-------------|-------------|
| F_FPR | 22.76% | **21.53%** | 22.36% |
| F_OAE | 34.93% | **23.29%** | 23.70% |
| F_DP | **2.91%** | 5.94% | 3.52% |
| F_MEO | 70.96% | **64.52%** | 69.14% |

#### DFD
| 지표 | Exp1 (Ep24) | Exp2 (Ep22) | Exp3 (Ep25) |
|------|-------------|-------------|-------------|
| F_FPR | **72.29%** | 84.19% | 67.33% |
| F_OAE | **34.69%** | 38.48% | 36.02% |
| F_DP | **22.82%** | 34.06% | 20.59% |
| F_MEO | **53.08%** | 69.65% | **49.44%** |

#### DFDC
| 지표 | Exp1 (Ep24) | Exp2 (Ep22) | Exp3 (Ep25) |
|------|-------------|-------------|-------------|
| F_FPR | **70.46%** | 81.56% | 76.53% |
| F_OAE | **24.73%** | 32.76% | 25.05% |
| F_DP | **28.48%** | 32.16% | 29.28% |
| F_MEO | 85.92% | 86.36% | **87.27%** |

---

## 4. 핵심 발견 및 분석

### 4.1 Gate 메커니즘 포화 문제 (가장 중요한 발견)

Cross-Attention Fusion의 Dynamic Gate는 Stage 1 특징과 Cross-Attention 결과를 동적으로 결합하는 메커니즘이다:
```
fused = gate * stage1_feat + (1 - gate) * cross_attn_result
```

| 실험 | gate_init_bias | Gate 범위 | Cross-Attn 기여도 |
|------|---------------|-----------|-------------------|
| Exp1 | 1.0 | ~0.96 (포화) | ~4% |
| Exp2 | 1.0 | ~0.99 (심각한 포화) | ~1% |
| Exp3 | **0.0** | **0.62-0.79** (동적) | **21-38%** |

**분석**:
- bias=1.0일 때 sigmoid(1.0)≈0.73에서 시작하여, 학습 중 gate가 0.96~0.99로 포화
- Cross-Attention Fusion이 사실상 비활성화되어, Stage 2 Adapter의 ~8.27M 파라미터가 제대로 활용되지 못함
- bias=0.0으로 변경 시 sigmoid(0.0)=0.5에서 균등 출발하여, 데이터 기반으로 최적 비율을 학습
- **Exp3에서 Gate가 0.62-0.79로 동적 변화**: Cross-Attention이 실질적으로 기여하는 최초의 실험

### 4.2 Sinkhorn Blur와 Fairness Loss의 상호작용 (두 번째 핵심 발견)

| 실험 | sinkhorn_blur | Fair Loss 크기 | lambda_fair | 실질 기여도 |
|------|--------------|----------------|-------------|------------|
| Exp1 | 1e-4 | ~0.07 | 0.05 | ~0.0035 (~1%) |
| Exp2 | 1e-3 | ~0.009 | 0.5 | ~0.0045 (~1%) |
| Exp3 | 1e-4 | ~0.07 | 0.15 | ~0.0105 (~3%) |

**분석**:
- sinkhorn_blur를 1e-4에서 1e-3으로 증가시키면 Sinkhorn distance 계산이 더 smooth해져 fair loss 값이 ~8배 축소
- Exp2에서 lambda_fair를 10배 증가했으나, fair loss 크기가 8배 줄어 실질 기여도는 Exp1과 거의 동일
- Exp3에서 blur를 복원하고 lambda_fair만 보수적으로 3배 증가하여 실질 기여도 3배 달성

### 4.3 Degenerate Phase (전체 동일 클래스 예측) 분석

| 실험 | Degenerate Phase | 원인 |
|------|-----------------|------|
| Exp1 | Ep 1-5 (5 epochs) | 정상 범위 |
| Exp2 | Ep 1-11 (11 epochs) | 과도한 정규화 + 낮은 lr |
| Exp3 | Ep 1-5 (5 epochs) | 정상 범위 복원 |

**분석**:
- 모든 실험에서 초기 epochs에서 모델이 다수 클래스(Real)만 예측 (Val ACC ≈ 0.8388 고정)
- Exp2에서 weight_decay 0.05, dropout 0.2, lr 0.0002의 조합이 수렴을 심각하게 지연
- Exp3에서 이 파라미터들을 복원하여 정상 수렴 속도 회복

### 4.4 Label Smoothing의 효과

- Exp1(미적용) → Exp3(0.1 적용) 비교에서 Val AUC는 소폭 감소 (0.8270 → 0.8232)
- 그러나 train loss와 val loss 간 gap이 줄어 overfitting이 다소 완화
- Exp3의 train loss (0.3833) > Exp1의 train loss (0.2737): 더 강한 정규화 효과
- label_smoothing 단독 효과는 미미하나, dropout=0.15와 조합 시 적정 수준의 정규화 달성

---

## 5. 실험별 종합 평가

### 5.1 Exp1 (Baseline) - 최고 탐지 성능
- **강점**: 최고 Val AUC (0.8270), 최고 cross-dataset 일반화, 빠른 수렴
- **약점**: Gate 포화로 Cross-Attention 비활성화, Fairness에 실질적 기여 미미
- **평가**: ★★★★☆ - 탐지 성능 기준으로 가장 우수하나, 모델 구조의 잠재력 미활용

### 5.2 Exp2 (Fairness Boost) - 실패한 공정성 강화
- **강점**: DFDC에서 약간 높은 AUC, label_smoothing 도입
- **약점**: 모든 주요 지표 하락, 11 epoch degenerate phase, 더 심한 gate 포화
- **교훈**: 여러 파라미터 동시 변경의 위험성 (sinkhorn_blur가 lambda_fair를 상쇄)
- **평가**: ★★☆☆☆ - 의도와 반대되는 결과, 그러나 핵심 인사이트 제공

### 5.3 Exp3 (Balanced Recovery) - 최적 균형
- **강점**:
  - Gate 포화 해결 (0.62-0.79 동적 범위) - 구조적 혁신
  - Val Fairness F_FPR **최선** (155.23%), F_MEO **최선** (87.75%)
  - DFD Test Fairness F_MEO **최선** (49.44%)
  - Val AUC 0.8232로 Exp1에 근접하면서 Fairness 개선
- **약점**: Exp1 대비 Val AUC -0.0038, Test AUC 소폭 하락
- **평가**: ★★★★★ - 탐지-공정성 최적 균형, Gate 문제 해결의 구조적 개선

---

## 6. 종합 결론

### 6.1 주요 성과

1. **Gate 초기화 문제 발견 및 해결**
   - bias=1.0에서 0.0으로 변경만으로 Cross-Attention 활용률 4% → 21-38%로 극적 개선
   - 이는 모델 구조의 근본적인 문제를 해결한 것으로, 향후 모든 실험에 적용 필요

2. **Sinkhorn Blur-Lambda 상호작용 규명**
   - blur와 lambda는 독립적으로 튜닝할 수 없는 결합 파라미터
   - blur 변경 시 fair loss 크기가 비선형적으로 변화하여 lambda 조정이 무효화됨

3. **Fairness 지표 개선**
   - Exp3에서 Val F_FPR 7% 개선 (166.89 → 155.23)
   - Exp3에서 Val F_MEO 4.4%p 개선 (92.17 → 87.75)
   - DFD Test F_MEO 3.6%p 개선 (53.08 → 49.44)

4. **성능 유지**
   - Val AUC 0.8232로 Baseline(0.8270) 대비 0.46% 감소에 불과
   - Cross-dataset 평균 AUC 0.7070으로 합리적 수준 유지

### 6.2 한계점

1. **Fairness 지표의 절대 수준**: F_FPR 155%, F_MEO 87%는 여전히 높은 편
2. **Val ACC 하락**: Exp1 0.8160 → Exp3 0.7772 (threshold 민감도 증가 가능성)
3. **Cross-dataset 일반화**: Exp1 대비 평균 1.7%p 하락
4. **Fairness-Performance Trade-off**: Val F_DP, F_OAE에서는 Exp1이 여전히 우수

### 6.3 향후 권장 사항

1. **gate_init_bias를 설정 파일로 분리**: 코드 직접 수정 대신 config에서 제어 가능하도록 리팩토링
2. **lambda_fair 0.15~0.30 범위 세밀 탐색**: Exp3 기반으로 더 공격적인 fairness 강화 가능
3. **Cosine Annealing with Warm Restart 주기 조정**: T_0=5, T_mult=2 → 더 긴 첫 주기 고려
4. **Threshold 최적화**: AUC가 유사한 수준에서 ACC를 높이기 위한 결정 임계값 튜닝
5. **sinkhorn_blur 1e-4~1e-3 사이 중간값 탐색**: 예) 3e-4, 5e-4에서 fair loss 크기 확인
6. **Cross-Attention 구조 변경**: Gate가 동적으로 작동하므로, attention head 수나 차원 변경 실험 가능

---

## 7. 파일 참조

| 구분 | 경로 |
|------|------|
| Exp1 Config | `config/train_stage2_independent_adapter.yaml` |
| Exp2 Config | `config/train_stage2_exp2.yaml` |
| Exp3 Config | `config/train_stage2_exp3.yaml` |
| Exp1 Logs | `logs/exp1_baseline_2026-01-28-10-44-38/` |
| Exp2 Logs | `logs/exp2_fairness_boost_2026-01-28-18-30-52/` |
| Exp3 Logs | `logs/exp3_balanced_recovery_2026-01-29-01-21-56/` |
| Exp1 Best Checkpoint | `logs/exp1_baseline_.../best_ep24_auc0.8270_acc0.8160.pth` |
| Exp2 Best Checkpoint | `logs/exp2_fairness_boost_.../best_ep22_auc0.8111_acc0.7893.pth` |
| Exp3 Best Checkpoint | `logs/exp3_balanced_recovery_.../best_ep25_auc0.8232_acc0.7772.pth` |
| Gate 수정 코드 | `model/cross_attention_fusion.py:65-66` |

---

*보고서 작성일: 2026-01-29*
*실험 기간: 2026-01-28 10:44 ~ 2026-01-29 08:39 (약 22시간)*
