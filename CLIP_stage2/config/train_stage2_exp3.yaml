# Stage 2 Experiment 3 Configuration
# Balanced Recovery: Exp1 성능 복원 + 실질적 Fairness 강화
#
# 실험 2 분석 결과 기반 변경사항 (Exp2→Exp3):
# 핵심 교훈: Exp2는 sinkhorn_blur 증가로 fair loss가 ~0.009로 축소되어
#           lambda_fair 10배 증가가 완전히 상쇄됨. 과도한 정규화가 수렴을 방해.
#
# 변경사항:
# 1. sinkhorn_blur: 1e-3 → 1e-4 (복원, 핵심! fair loss 크기 복원)
# 2. weight_decay: 0.05 → 0.01 (복원, 과정규화 해소)
# 3. lr: 0.0002 → 0.0003 (복원, 수렴 속도 회복)
# 4. warmup_epochs: 2 → 3 (복원, 안정적 초기화)
# 5. num_epochs: 25 → 30 (복원, 충분한 학습)
# 6. dropout: 0.2 → 0.15 (절충, label_smoothing과 균형)
# 7. lambda_fair: 0.5 → 0.15 (보수적 3배, sinkhorn_blur 복원으로 실질 기여 3배)
# 8. label_smoothing: 0.1 유지 (일반화 성능 개선 효과)
#
# 추가 코드 변경: gate_init_bias 1.0 → 0.0 (cross_attention_fusion.py)
#
# 예상 실질 fairness loss 기여도:
# - Exp1: 0.05 * 0.07 = 0.0035 (~1% of total loss)
# - Exp2: 0.5 * 0.009 = 0.0045 (~1% of total loss) -- blur로 상쇄
# - Exp3: 0.15 * 0.07 = 0.0105 (~3% of total loss) -- 실질 3배 증가

# ==================== Model Configuration ====================
model:
  clip_name: "ViT-L/14"
  feature_dim: 768

  # Stage 1 Adapter (Frozen): Global Fairness 보정
  stage1_hidden_dim: 512

  # Stage 2 Adapter (Trainable): Detection + Local Fairness
  stage2_hidden_dim: 384

  # Cross-Attention Fusion
  fusion_num_heads: 8

  # Binary Classifier
  classifier_hidden_dims: [384, 192]
  num_classes: 2

  # 기타
  dropout: 0.15              # 변경: 0.2 → 0.15 (Exp1=0.1과 Exp2=0.2의 절충)
  normalize_features: true

# Stage 1 Checkpoint (Additive Adapter 가중치)
stage1_checkpoint: "/workspace/code/CLIP_stage1/logs/stage1_2026-01-20-12-31-57/checkpoint_best.pth"

# ==================== Dataset Configuration ====================
dataset:
  fairness_root: "/workspace/datasets/fairness"
  train_dataset: ["ff++"]
  validation_dataset: ["ff++"]
  test_dataset: ["celebdf", "dfd", "dfdc"]
  resolution: 256
  use_data_augmentation: true

  # Subgroup 균형 샘플링
  use_balance_sampling: false       # Subgroup 내 Real/Fake 균형 (기존)
  use_subgroup_balance: false        # Subgroup 간 균형 (Dataset-level)
  use_subgroup_sampler: true        # Batch-level subgroup 균형
  skip_unknown_attributes: false

# Data normalization (ImageNet default)
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# ==================== Training Configuration ====================
training:
  num_epochs: 30              # 복원: 25 → 30 (충분한 학습 시간 확보)
  train_batch_size: 256
  val_batch_size: 64
  num_workers: 4

# ==================== Optimizer Configuration ====================
optimizer:
  type: "adamw"
  lr: 0.0003                  # 복원: 2e-4 → 3e-4 (수렴 속도 회복)
  weight_decay: 0.01          # 복원: 0.05 → 0.01 (과정규화 해소, 핵심!)
  betas: [0.9, 0.999]
  eps: 1.0e-8

# ==================== Scheduler Configuration ====================
scheduler:
  use_scheduler: true
  name: "WarmupCosine"
  T_0: 5
  T_mult: 2
  eta_min: 1.0e-6

  # WarmupCosine 옵션
  warmup_epochs: 3            # 복원: 2 → 3 (안정적 초기화)

# ==================== Loss Configuration ====================
loss:
  type: "cross_entropy"
  focal_alpha: 0.25
  focal_gamma: 2.0
  label_smoothing: 0.1        # 유지: Exp2에서 추가된 일반화 개선 효과
  class_weights: null

# Fairness Loss Configuration
fairness_loss:
  sinkhorn_blur: 1.0e-4       # 복원: 1e-3 → 1e-4 (핵심! fair loss 크기 복원)
  sinkhorn_p: 2
  scaling: 0.9
  min_samples_per_subgroup: 2
  num_subgroups: 8

# Loss Weighting Configuration
loss_weighting:
  type: "fixed"
  # Dynamic weighting 옵션
  init_log_var_cls: 0.0
  init_log_var_fair: 0.0
  # Fixed weighting 옵션
  lambda_cls: 1.0
  lambda_fair: 0.15           # 변경: 0.5 → 0.15 (보수적 3배, blur 복원으로 실질 기여 3배)

# Gradient Clipping
gradient_clip_max_norm: 1.0

# ==================== Logging Configuration ====================
logging:
  log_dir: "/workspace/code/CLIP_stage2/logs"
  experiment_name: "exp3_balanced_recovery"
  print_freq: 50
  save_freq: 1

# ==================== Evaluation Configuration ====================
evaluation:
  eval_freq: 1
  save_best_only: true
  metric_for_best: "auc"
  test_every_n_epochs: 5
  test_datasets: ["celebdf", "dfd", "dfdc"]

# ==================== Hardware Configuration ====================
device: "cuda"
seed: 42
deterministic: true

# CLIP weights download path
clip_download_root: "/data/cuixinjie/weights"

# ==================== Expected Trainable Parameters ====================
# Component          | Status    | Parameters
# -------------------|-----------|------------
# CLIP Encoder       | Frozen    | ~303M
# Stage 1 Adapter    | Frozen    | ~1.2M
# Stage 2 Adapter    | Trainable | ~600K
# Cross-Attn Fusion  | Trainable | ~2.4M
# Dynamic Gate       | Trainable | ~1K
# Loss Weighting     | Trainable | 2 params
# Binary Classifier  | Trainable | ~400K
# -------------------|-----------|------------
# Total Trainable    |           | ~3.4M
