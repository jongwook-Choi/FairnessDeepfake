"""
Intersection Fairness Metrics ê³„ì‚° ëª¨ë“ˆ
Fairness-Generalization í”„ë¡œì íŠ¸ ë°©ì‹ê³¼ ìœ ì‚¬í•œ êµì°¨ ê·¸ë£¹ ë¶„ì„ êµ¬í˜„

ì£¼ìš” ê¸°ëŠ¥:
1. Intersection ê·¸ë£¹ ìƒì„± (ì˜ˆ: male,asian, female,white)
2. Inter-groupê³¼ Intra-group fairness ë™ì‹œ ê³„ì‚°
3. ê¸°ì¡´ fairness_metrics_advancedì™€ í˜¸í™˜ ê°€ëŠ¥í•œ ì¸í„°í˜ì´ìŠ¤
"""

import numpy as np
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score
from collections import defaultdict
import itertools


def classification_metrics(label, prediction):
    """
    ê¸°ë³¸ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°

    Args:
        label: Ground truth labels (0 or 1)
        prediction: Predicted probabilities (0~1)

    Returns:
        tuple: ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ë“¤
    """
    fpr, tpr, threshold = roc_curve(label, prediction, pos_label=1)
    fnr = 1 - tpr
    eer_threshold = threshold[np.nanargmin(np.absolute(fnr - fpr))]
    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]

    auc = roc_auc_score(label, prediction)
    acc = accuracy_score(label, prediction >= 0.5)
    precision = precision_score(label, prediction >= 0.5, zero_division=0)

    CM = confusion_matrix(label, prediction >= 0.5)
    TN = CM[0][0] if len(CM) > 0 else 0
    FN = CM[1][0] if len(CM) > 1 else 0
    TP = CM[1][1] if len(CM) > 1 else 0
    FP = CM[0][1] if len(CM) > 0 else 0

    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0
    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0
    FNR = FN / (FN + TP) if (FN + TP) > 0 else 0
    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0

    f_g = (TP + FP) / len(label) if len(label) > 0 else 0
    positive_rate = (TP + FP) / len(label) if len(label) > 0 else 0
    negative_rate = (TN + FN) / len(label) if len(label) > 0 else 0

    er = 1 - ((TPR + (1 - FPR)) / 2)

    return auc, er, FPR, TPR, acc, precision, f_g, eer, positive_rate, negative_rate, FNR, TNR


def compute_fairness_metrics_intersection(predictions_dict, labels_dict, attribute_groups):
    """
    Intersection Fairness Metrics ê³„ì‚° (êµì°¨ ê·¸ë£¹ ë¶„ì„ í¬í•¨)

    Args:
        predictions_dict: dict[key] -> np.array of predictions
                         ì˜ˆ: {'gender_male': [...], 'gender_female': [...],
                              'race_asian': [...], 'race_white': [...]}
        labels_dict: dict[key] -> np.array of labels
        attribute_groups: list of attribute group names
                         ì˜ˆ: ['gender', 'race']

    Returns:
        dict: {
            'single_group_metrics': {...},      # ë‹¨ì¼ ì„œë¸Œê·¸ë£¹ ë©”íŠ¸ë¦­
            'intersection_metrics': {...},      # êµì°¨ ê·¸ë£¹ ë©”íŠ¸ë¦­
            'group_overall_metrics': {...},     # ê·¸ë£¹ë³„ ì „ì²´ ë©”íŠ¸ë¦­
            'intra_fairness': {...},           # Intra-group fairness
            'inter_fairness': {...},           # Inter-group fairness
            'overall_fairness': {...},         # ì¢…í•© fairness ë©”íŠ¸ë¦­
            'skipped_groups': {...}            # ê³„ì‚° ì‹¤íŒ¨í•œ ê·¸ë£¹ ì •ë³´
        }
    """

    # Step 1: ë‹¨ì¼ ì„œë¸Œê·¸ë£¹ ë©”íŠ¸ë¦­ ê³„ì‚° (ê¸°ì¡´ê³¼ ë™ì¼)
    single_metrics = {}
    skipped_groups = {}  # ê³„ì‚° ì‹¤íŒ¨í•œ ê·¸ë£¹ê³¼ ì´ìœ  ì €ì¥

    for key in predictions_dict:
        if len(labels_dict[key]) == 0:
            continue

        try:
            auc, er, fpr, tpr, acc, precision, f_g, eer, pr, nr, fnr, tnr = classification_metrics(
                labels_dict[key], predictions_dict[key]
            )

            single_metrics[key] = {
                'auc': auc,
                'error_rate': er,
                'FPR': fpr,
                'TPR': tpr,
                'acc': acc,
                'precision': precision,
                'F_G': f_g,
                'eer': eer,
                'positive_rate': pr,
                'negative_rate': nr,
                'FNR': fnr,
                'TNR': tnr,
                'num_samples': len(labels_dict[key])
            }
        except Exception as e:
            # ê³„ì‚° ì‹¤íŒ¨í•œ ê·¸ë£¹ ì •ë³´ ì €ì¥
            error_msg = str(e)
            num_samples = len(labels_dict[key])

            # ë¼ë²¨ ë¶„í¬ í™•ì¸
            unique_labels = np.unique(labels_dict[key])
            label_counts = {int(label): int(np.sum(labels_dict[key] == label)) for label in unique_labels}

            skipped_groups[key] = {
                'reason': error_msg,
                'num_samples': num_samples,
                'label_distribution': label_counts,
                'error_type': type(e).__name__
            }

            print(f"Warning: Skipping {key} - {error_msg} (samples={num_samples}, labels={label_counts})")
            continue

    # Step 2: Intersection ê·¸ë£¹ ë©”íŠ¸ë¦­ ê³„ì‚° (ì´ë¯¸ ìƒì„±ëœ intersection ë°ì´í„° ì‚¬ìš©)
    intersection_metrics = {}

    # predictions_dictì—ì„œ ì´ë¯¸ intersectionì¸ í•­ëª© ì¶”ì¶œ (ì‰¼í‘œê°€ ìˆëŠ” key)
    for key in predictions_dict:
        if ',' in key:  # Intersection í‚¤ (ì˜ˆ: gender_male,race_asian)
            # ì´ë¯¸ single_metricsì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ intersection_metricsë¡œ ë¶„ë¦¬
            if key in single_metrics:
                intersection_metrics[key] = single_metrics[key]

    # Step 3: Groupë³„ ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
    group_metrics = {}
    for group_name in attribute_groups:
        group_preds = []
        group_labels = []

        for key in predictions_dict:
            if key.startswith(f"{group_name}_"):
                group_preds.extend(predictions_dict[key])
                group_labels.extend(labels_dict[key])

        if len(group_labels) > 0:
            try:
                auc, er, fpr, tpr, acc, precision, f_g, eer, pr, nr, fnr, tnr = classification_metrics(
                    np.array(group_labels), np.array(group_preds)
                )

                group_metrics[group_name] = {
                    'auc': auc,
                    'error_rate': er,
                    'FPR': fpr,
                    'TPR': tpr,
                    'acc': acc,
                    'precision': precision,
                    'F_G': f_g,
                    'eer': eer,
                    'positive_rate': pr,
                    'negative_rate': nr,
                    'FNR': fnr,
                    'TNR': tnr,
                }
            except Exception as e:
                print(f"Warning: Failed to compute group metrics for {group_name}: {e}")

    # Step 4: Intra-group Fairness ê³„ì‚° (ë‹¨ì¼ ê·¸ë£¹ ë‚´ ê³µì •ì„±)
    intra_fairness = {}

    for group_name in attribute_groups:
        subgroup_metrics = {}
        for key, value in single_metrics.items():
            # ë‹¨ì¼ ê·¸ë£¹ë§Œ ì‚¬ìš© (intersection ê·¸ë£¹ ì œì™¸)
            if key.startswith(f"{group_name}_") and ',' not in key:
                subgroup_name = key.replace(f"{group_name}_", "")
                subgroup_metrics[subgroup_name] = value

        if len(subgroup_metrics) >= 2:
            fprs = [m['FPR'] for m in subgroup_metrics.values()]
            tprs = [m['TPR'] for m in subgroup_metrics.values()]
            fnrs = [m['FNR'] for m in subgroup_metrics.values()]
            tnrs = [m['TNR'] for m in subgroup_metrics.values()]
            accs = [m['acc'] for m in subgroup_metrics.values()]
            prs = [m['positive_rate'] for m in subgroup_metrics.values()]
            nrs = [m['negative_rate'] for m in subgroup_metrics.values()]

            # Intra-group Overall FPR/TPR ê³„ì‚° (ì „ì²´ ìƒ˜í”Œ ê°€ì¤‘ â€” PG-FDD ë°©ì‹)
            group_all_labels = []
            group_all_preds = []
            for key in predictions_dict:
                if key.startswith(f"{group_name}_") and ',' not in key:
                    group_all_labels.extend(labels_dict[key])
                    group_all_preds.extend(predictions_dict[key])

            # Overall FPR/TPR (ê°€ì¤‘ í‰ê· )
            if len(group_all_labels) > 0:
                try:
                    _, _, overall_fpr_weighted, overall_tpr_weighted, _, _, _, _, _, _, _, _ = \
                        classification_metrics(np.array(group_all_labels), np.array(group_all_preds))
                except Exception:
                    overall_fpr_weighted = np.mean(fprs)
                    overall_tpr_weighted = np.mean(tprs)
            else:
                overall_fpr_weighted = np.mean(fprs)
                overall_tpr_weighted = np.mean(tprs)

            # Intra-group fairness metrics
            # F_FPR (Mean): ì„œë¸Œê·¸ë£¹ FPR í‰ê· ê³¼ì˜ ì°¨ì´ í•© (ë¹„ê°€ì¤‘)
            mean_fpr = np.mean(fprs)
            intra_fairness[f'{group_name}_F_FPR'] = sum([abs(fpr - mean_fpr) for fpr in fprs]) * 100

            # F_FPR_overall: ì „ì²´ ìƒ˜í”Œ overall FPRê³¼ì˜ ì°¨ì´ í•© (ê°€ì¤‘, PG-FDD ë°©ì‹)
            intra_fairness[f'{group_name}_F_FPR_overall'] = \
                sum([abs(fpr - overall_fpr_weighted) for fpr in fprs]) * 100

            # F_FPR_maxmin: max-min ë°©ì‹
            intra_fairness[f'{group_name}_F_FPR_maxmin'] = (max(fprs) - min(fprs)) * 100

            intra_fairness[f'{group_name}_F_OAE'] = (max(accs) - min(accs)) * 100
            intra_fairness[f'{group_name}_F_DP'] = max(max(prs) - min(prs), max(nrs) - min(nrs)) * 100
            intra_fairness[f'{group_name}_F_MEO'] = max(
                max(fprs) - min(fprs),
                max(fnrs) - min(fnrs),
                max(tnrs) - min(tnrs),
                max(tprs) - min(tprs)
            ) * 100

            # F_EO: Equalized Odds (PG-FDD ë°©ì‹) â€” FPR+TPR ì°¨ì´ í•©ê³„
            intra_fairness[f'{group_name}_F_EO'] = \
                sum([abs(fpr - overall_fpr_weighted) + abs(tpr - overall_tpr_weighted)
                     for fpr, tpr in zip(fprs, tprs)]) * 100

    # Step 5: Inter-group Fairness ê³„ì‚° (êµì°¨ ê·¸ë£¹ ê°„ ê³µì •ì„±)
    inter_fairness = {}

    if len(intersection_metrics) >= 2:
        # ëª¨ë“  intersection ê·¸ë£¹ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        inter_fprs = [m['FPR'] for m in intersection_metrics.values()]
        inter_tprs = [m['TPR'] for m in intersection_metrics.values()]
        inter_fnrs = [m['FNR'] for m in intersection_metrics.values()]
        inter_tnrs = [m['TNR'] for m in intersection_metrics.values()]
        inter_accs = [m['acc'] for m in intersection_metrics.values()]
        inter_prs = [m['positive_rate'] for m in intersection_metrics.values()]
        inter_nrs = [m['negative_rate'] for m in intersection_metrics.values()]

        if len(inter_fprs) >= 2:
            # Inter-group Overall FPR/TPR ê³„ì‚° (ì „ì²´ ìƒ˜í”Œ ê°€ì¤‘ â€” PG-FDD ë°©ì‹)
            all_inter_labels = []
            all_inter_preds = []
            for key in intersection_metrics:
                if key in predictions_dict:
                    all_inter_labels.extend(labels_dict[key])
                    all_inter_preds.extend(predictions_dict[key])

            if len(all_inter_labels) > 0:
                try:
                    _, _, overall_inter_fpr_weighted, overall_inter_tpr_weighted, \
                        _, _, _, _, _, _, _, _ = \
                        classification_metrics(np.array(all_inter_labels), np.array(all_inter_preds))
                except Exception:
                    overall_inter_fpr_weighted = np.mean(inter_fprs)
                    overall_inter_tpr_weighted = np.mean(inter_tprs)
            else:
                overall_inter_fpr_weighted = np.mean(inter_fprs)
                overall_inter_tpr_weighted = np.mean(inter_tprs)

            # Inter-group fairness metrics
            # F_FPR (Mean): ì„œë¸Œê·¸ë£¹ FPR í‰ê· ê³¼ì˜ ì°¨ì´ í•© (ë¹„ê°€ì¤‘)
            mean_inter_fpr = np.mean(inter_fprs)
            inter_fairness['inter_F_FPR'] = sum([abs(fpr - mean_inter_fpr) for fpr in inter_fprs]) * 100

            # F_FPR_overall: ì „ì²´ ìƒ˜í”Œ overall FPRê³¼ì˜ ì°¨ì´ í•© (ê°€ì¤‘, PG-FDD ë°©ì‹)
            inter_fairness['inter_F_FPR_overall'] = \
                sum([abs(fpr - overall_inter_fpr_weighted) for fpr in inter_fprs]) * 100

            # F_FPR_maxmin: max-min ë°©ì‹
            inter_fairness['inter_F_FPR_maxmin'] = (max(inter_fprs) - min(inter_fprs)) * 100

            inter_fairness['inter_F_OAE'] = (max(inter_accs) - min(inter_accs)) * 100
            inter_fairness['inter_F_DP'] = max(
                max(inter_prs) - min(inter_prs),
                max(inter_nrs) - min(inter_nrs)
            ) * 100
            inter_fairness['inter_F_MEO'] = max(
                max(inter_fprs) - min(inter_fprs),
                max(inter_fnrs) - min(inter_fnrs),
                max(inter_tnrs) - min(inter_tnrs),
                max(inter_tprs) - min(inter_tprs)
            ) * 100

            # F_EO: Equalized Odds (PG-FDD ë°©ì‹) â€” FPR+TPR ì°¨ì´ í•©ê³„
            inter_fairness['inter_F_EO'] = \
                sum([abs(fpr - overall_inter_fpr_weighted) + abs(tpr - overall_inter_tpr_weighted)
                     for fpr, tpr in zip(inter_fprs, inter_tprs)]) * 100

    # Step 6: ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (F_S, F_A_inter ë“±)
    overall_fairness = {}

    # F_S ê³„ì‚° (Statistical Parity ì°¨ì´)
    if len(intersection_metrics) >= 2:
        f_g_values = [m['F_G'] for m in intersection_metrics.values()]
        if len(f_g_values) >= 2:
            # ëª¨ë“  ìŒì˜ ì°¨ì´ ì¤‘ ìµœëŒ€ê°’
            max_diff = 0
            for i in range(len(f_g_values)):
                for j in range(i+1, len(f_g_values)):
                    diff = abs(f_g_values[i] - f_g_values[j])
                    if diff > max_diff:
                        max_diff = diff
            overall_fairness['F_S'] = max_diff * 100

    # F_A_inter ê³„ì‚° (í‰ê·  inter-group ì°¨ì´)
    if 'F_S' in overall_fairness and len(group_metrics) > 0:
        # ê° intersectionê³¼ ì „ì²´ í‰ê· ì˜ ì°¨ì´ ê³„ì‚°
        overall_f_g = np.mean([m['F_G'] for m in group_metrics.values()])
        inter_diffs = []
        for metrics in intersection_metrics.values():
            inter_diffs.append(abs(metrics['F_G'] - overall_f_g))

        if inter_diffs:
            max_inter_diff = max(inter_diffs)
            overall_fairness['F_A_inter'] = (overall_fairness['F_S'] + max_inter_diff * 100) / 2

    # single_group_metricsì—ì„œ intersection ì œì™¸ (ì‰¼í‘œê°€ ì—†ëŠ” í‚¤ë§Œ)
    single_group_only = {k: v for k, v in single_metrics.items() if ',' not in k}

    return {
        'single_group_metrics': single_group_only,
        'intersection_metrics': intersection_metrics,
        'group_overall_metrics': group_metrics,
        'intra_fairness': intra_fairness,
        'inter_fairness': inter_fairness,
        'overall_fairness': overall_fairness,
        'skipped_groups': skipped_groups  # ê³„ì‚° ì‹¤íŒ¨í•œ ê·¸ë£¹ ì •ë³´
    }


def compute_fairness_with_indices(data_with_indices, attribute_columns, target_column='label',
                                 prediction_column='prediction'):
    """
    ì¸ë±ìŠ¤ ê¸°ë°˜ ë°ì´í„°ë¡œ ì •í™•í•œ intersection ê³„ì‚°

    Args:
        data_with_indices: DataFrame ë˜ëŠ” dict with columns:
                          - prediction: ì˜ˆì¸¡ê°’
                          - label: ì‹¤ì œ ë¼ë²¨
                          - gender: ì„±ë³„ (male/female)
                          - race: ì¸ì¢… (asian/white/black/others)
                          - ê¸°íƒ€ attributes
        attribute_columns: list of attribute column names ['gender', 'race']
        target_column: ë¼ë²¨ ì»¬ëŸ¼ëª…
        prediction_column: ì˜ˆì¸¡ ì»¬ëŸ¼ëª…

    Returns:
        ì™„ì „í•œ fairness ë¶„ì„ ê²°ê³¼
    """
    import pandas as pd

    # DataFrameìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
    if isinstance(data_with_indices, dict):
        df = pd.DataFrame(data_with_indices)
    else:
        df = data_with_indices.copy()

    # ë‹¨ì¼ ê·¸ë£¹ë³„ ë°ì´í„° ë¶„ë¦¬
    single_preds = {}
    single_labels = {}

    for attr in attribute_columns:
        unique_values = df[attr].unique()
        for value in unique_values:
            key = f"{attr}_{value}"
            mask = df[attr] == value
            single_preds[key] = df.loc[mask, prediction_column].values
            single_labels[key] = df.loc[mask, target_column].values

    # Intersection ê·¸ë£¹ë³„ ë°ì´í„° ë¶„ë¦¬
    intersection_preds = {}
    intersection_labels = {}

    # 2ê°œ attribute intersectionë§Œ ê³„ì‚° (gender Ã— race)
    if len(attribute_columns) >= 2:
        for attr1, attr2 in itertools.combinations(attribute_columns, 2):
            for val1 in df[attr1].unique():
                for val2 in df[attr2].unique():
                    key = f"{attr1}_{val1},{attr2}_{val2}"
                    mask = (df[attr1] == val1) & (df[attr2] == val2)
                    if mask.sum() > 0:
                        intersection_preds[key] = df.loc[mask, prediction_column].values
                        intersection_labels[key] = df.loc[mask, target_column].values

    # 3-way ì´ìƒì˜ intersectionì€ ê³„ì‚°í•˜ì§€ ì•ŠìŒ (2-wayë§Œ ì‚¬ìš©)
    # ì´ìœ : gender Ã— race ì¡°í•©ë§Œ í•„ìš” (gender_male,race_asian ë“±)

    # í†µí•© ë”•ì…”ë„ˆë¦¬ ìƒì„±
    all_preds = {**single_preds, **intersection_preds}
    all_labels = {**single_labels, **intersection_labels}

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    return compute_fairness_metrics_intersection(all_preds, all_labels, attribute_columns)


def print_intersection_fairness_report(results):
    """
    Intersection Fairness ë¦¬í¬íŠ¸ ì¶œë ¥

    Args:
        results: compute_fairness_metrics_intersection ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ğŸ“Š INTERSECTION FAIRNESS METRICS REPORT")
    print("="*80)

    # ë‹¨ì¼ ì„œë¸Œê·¸ë£¹ ì„±ëŠ¥
    if 'single_group_metrics' in results:
        print("\nğŸ”¹ SINGLE SUBGROUP PERFORMANCE")
        print("-" * 80)
        print(f"{'Group':<25} {'Samples':>8} {'AUC':>8} {'ACC':>8} {'EER':>8} {'FPR':>8} {'TPR':>8}")
        print("-" * 80)
        for key, metrics in results['single_group_metrics'].items():
            print(f"{key:<25} {metrics['num_samples']:>8} "
                  f"{metrics['auc']:>8.4f} {metrics['acc']:>8.4f} {metrics['eer']:>8.4f} "
                  f"{metrics['FPR']:>8.4f} {metrics['TPR']:>8.4f}")

    # Intersection ê·¸ë£¹ ì„±ëŠ¥
    if 'intersection_metrics' in results and results['intersection_metrics']:
        print("\nğŸ”¸ INTERSECTION GROUP PERFORMANCE (Gender Ã— Race)")
        print("-" * 80)
        print(f"{'Group':<35} {'Samples':>8} {'AUC':>8} {'ACC':>8} {'EER':>8} {'FPR':>8} {'TPR':>8}")
        print("-" * 80)
        for key, metrics in results['intersection_metrics'].items():
            print(f"{key:<35} {metrics['num_samples']:>8} "
                  f"{metrics['auc']:>8.4f} {metrics['acc']:>8.4f} {metrics['eer']:>8.4f} "
                  f"{metrics['FPR']:>8.4f} {metrics['TPR']:>8.4f}")

    # Intra-group Fairness
    if 'intra_fairness' in results and results['intra_fairness']:
        print("\nâš–ï¸  INTRA-GROUP FAIRNESS (Within Single Attribute)")
        print("-" * 80)
        groups = sorted(set([k.split('_')[0] for k in results['intra_fairness'].keys() if not k.endswith('_maxmin')]))

        print(f"{'Attribute':<12} {'F_FPR(Mean)':>13} {'F_FPR(Over)':>13} {'F_FPR(MM)':>12} {'F_OAE':>9} {'F_DP':>9} {'F_MEO':>9} {'F_EO':>9}")
        print("-" * 100)
        for group in groups:
            f_fpr = results['intra_fairness'].get(f'{group}_F_FPR', 0)
            f_fpr_ov = results['intra_fairness'].get(f'{group}_F_FPR_overall', 0)
            f_fpr_mm = results['intra_fairness'].get(f'{group}_F_FPR_maxmin', 0)
            f_oae = results['intra_fairness'].get(f'{group}_F_OAE', 0)
            f_dp = results['intra_fairness'].get(f'{group}_F_DP', 0)
            f_meo = results['intra_fairness'].get(f'{group}_F_MEO', 0)
            f_eo = results['intra_fairness'].get(f'{group}_F_EO', 0)
            print(f"{group.upper():<12} {f_fpr:>12.3f}% {f_fpr_ov:>12.3f}% {f_fpr_mm:>11.3f}% {f_oae:>8.3f}% {f_dp:>8.3f}% {f_meo:>8.3f}% {f_eo:>8.3f}%")

    # Inter-group Fairness
    if 'inter_fairness' in results and results['inter_fairness']:
        print("\nâš–ï¸  INTER-GROUP FAIRNESS (Across All Intersections)")
        print("-" * 80)
        inter = results['inter_fairness']
        print(f"{'Metric':<20} {'Value':>12}")
        print("-" * 80)
        if 'inter_F_FPR' in inter:
            print(f"{'F_FPR (Mean)':<20} {inter['inter_F_FPR']:>11.3f}%")
        if 'inter_F_FPR_overall' in inter:
            print(f"{'F_FPR (Overall)':<20} {inter['inter_F_FPR_overall']:>11.3f}%")
        if 'inter_F_FPR_maxmin' in inter:
            print(f"{'F_FPR (MaxMin)':<20} {inter['inter_F_FPR_maxmin']:>11.3f}%")
        for key, value in inter.items():
            if key not in ['inter_F_FPR', 'inter_F_FPR_overall', 'inter_F_FPR_maxmin']:
                metric_name = key.replace('inter_', 'F_')
                print(f"{metric_name:<20} {value:>11.3f}%")

    # Overall Fairness
    if 'overall_fairness' in results and results['overall_fairness']:
        print("\nğŸ“ˆ OVERALL FAIRNESS METRICS")
        print("-" * 80)
        print(f"{'Metric':<25} {'Value':>12}")
        print("-" * 80)
        for key, value in results['overall_fairness'].items():
            print(f"{key:<25} {value:>11.3f}%")

    # Fairness ë¹„êµ (Intra vs Inter)
    if 'intra_fairness' in results and 'inter_fairness' in results:
        print("\nğŸ” FAIRNESS COMPARISON (Intra vs Inter)")
        print("-" * 80)

        # í‰ê·  ê³„ì‚°
        intra_values = [v for k, v in results['intra_fairness'].items() if 'F_FPR' in k]
        inter_value = results['inter_fairness'].get('inter_F_FPR', 0)

        if intra_values:
            avg_intra = np.mean(intra_values)
            print(f"{'Metric':<30} {'Value':>12}")
            print("-" * 80)
            print(f"{'Average Intra-group F_FPR':<30} {avg_intra:>11.3f}%")
            print(f"{'Inter-group F_FPR':<30} {inter_value:>11.3f}%")
            print("-" * 80)

            if inter_value > avg_intra:
                ratio = inter_value / avg_intra
                print(f"âš ï¸  Inter-group bias is {ratio:.1f}x higher than intra-group")
            else:
                print(f"âœ“ Inter-group fairness is comparable to intra-group")

    # Skipped Groups ì •ë³´ ì¶œë ¥
    if 'skipped_groups' in results and results['skipped_groups']:
        print("\nâš ï¸  SKIPPED GROUPS (Computation Failed)")
        print("-" * 80)
        print(f"{'Group':<35} {'Samples':>8} {'Label Dist':>20} {'Error':>15}")
        print("-" * 80)
        for group_name, info in results['skipped_groups'].items():
            label_dist = str(info['label_distribution'])
            if len(label_dist) > 20:
                label_dist = label_dist[:17] + "..."
            print(f"{group_name:<35} {info['num_samples']:>8} {label_dist:>20} {info['error_type']:>15}")
            print(f"  â””â”€ Reason: {info['reason']}")

    print("\n" + "="*80)


def backward_compatible_wrapper(predictions_dict, labels_dict, attribute_groups,
                               use_intersection=False):
    """
    ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜

    Args:
        predictions_dict: ì˜ˆì¸¡ê°’ ë”•ì…”ë„ˆë¦¬
        labels_dict: ë¼ë²¨ ë”•ì…”ë„ˆë¦¬
        attribute_groups: ì†ì„± ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸
        use_intersection: Trueë©´ intersection ë¶„ì„, Falseë©´ ê¸°ì¡´ ë°©ì‹

    Returns:
        ê¸°ì¡´ í˜•ì‹ê³¼ í˜¸í™˜ë˜ëŠ” ê²°ê³¼
    """
    if not use_intersection:
        # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ë‹¨ì¼ ì„œë¸Œê·¸ë£¹ë§Œ)
        from utils.fairness_metrics_advanced import compute_fairness_metrics_advanced
        return compute_fairness_metrics_advanced(predictions_dict, labels_dict, attribute_groups)

    # Intersection ë¶„ì„ ì‚¬ìš©
    results = compute_fairness_metrics_intersection(predictions_dict, labels_dict, attribute_groups)

    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    backward_results = {
        'per_group_metrics': results['single_group_metrics'],
        'group_overall_metrics': results['group_overall_metrics'],
        'fairness_metrics': {}
    }

    # Fairness metrics í†µí•©
    backward_results['fairness_metrics'].update(results.get('intra_fairness', {}))

    # Inter fairnessë¥¼ ì¶”ê°€ (ì ‘ë‘ì‚¬ ìœ ì§€)
    if 'inter_fairness' in results:
        backward_results['fairness_metrics'].update(results['inter_fairness'])

    # Overall fairness ì¶”ê°€
    if 'overall_fairness' in results:
        backward_results['fairness_metrics'].update(results['overall_fairness'])

    return backward_results


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì˜ˆì œ
    print("Intersection Fairness Metrics Module")
    print("\nUsage Examples:")
    print("\n1. Basic intersection analysis:")
    print("  from utils.fairness_metrics_intersection import compute_fairness_metrics_intersection")
    print("  results = compute_fairness_metrics_intersection(preds_dict, labels_dict, ['gender', 'race'])")
    print("  print_intersection_fairness_report(results)")

    print("\n2. With DataFrame input (accurate intersection):")
    print("  from utils.fairness_metrics_intersection import compute_fairness_with_indices")
    print("  df = pd.DataFrame({")
    print("      'prediction': [...],")
    print("      'label': [...],")
    print("      'gender': ['male', 'female', ...],")
    print("      'race': ['asian', 'white', ...]")
    print("  })")
    print("  results = compute_fairness_with_indices(df, ['gender', 'race'])")

    print("\n3. Backward compatible mode:")
    print("  from utils.fairness_metrics_intersection import backward_compatible_wrapper")
    print("  results = backward_compatible_wrapper(preds_dict, labels_dict, ['gender', 'race'], use_intersection=True)")